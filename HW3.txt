#### HW3 ####

ssh team@176.109.91.27 # ssh login
ssh team-25-nn # nn login
- enter pwd

su team
sudo apt install postgresql #install to nn from user team
-enter pwd
yes

sudo -i -u postgres # switch to newly created user
psql #connect to postgres terminal

## create db, create user and give it all privileges and make it db owner ##

CREATE DATABASE metastore;
CREATE USER hive with password 'any_pass_you_like';
GRANT ALL PRIVILEGES ON DATABASE "metastore" TO hive; 
ALTER DATABASE metastore OWNER TO hive;

\q #quit postgres terminal
exit #get back to user team
su team


## CONFIG POSTGRESQL ##
sudo nano /etc/postgresql/16/main/postgresql.conf
- add line 'listen_adresses = 'team-25-nn' above line 'listen_adresses = 'localhost'# (CONNECTION AND AUTHENTICATION section)
- quit vim/nano


sudo nano /etc/postgresql/16/main/pg_hba.conf
- find "IPv4 local connections" part:
- paste following lines:
host    metastore       hive            192.168.1.0/24          scram-sha-256
host    metastore       hive            127.0.0.1/32            scram-sha-256
host    all             all             127.0.0.1/32            scram-sha-256 # host metastore hive 
- quit nano

## restart postgres and and go back to jn ##
sudo systemctl restart postgresql
sudo systemctl status postgresql
exit 
ssh team-25-jn #back to jn

su team
sudo apt install postgresql-client-16 #install to jn from user team
enter pwd
y

su hadoop
## Create tunnel as hadoop user ##
ssh -L 5432:team-25-nn:5432 hadoop@team-25-nn -N &

psql -h localhost -p 5432 -U hive -W -d metastore
\q #leave metastore


## HIVE INSTALLATION ##

su hadoop
cd ~
wget https://dlcdn.apache.org/hive/hive-4.0.1/apache-hive-4.0.1-bin.tar.gz
tar -xvzf apache-hive-4.0.1-bin.tar.gz
cd apache-hive-4.0.1-bin/
cd lib
wget https://jdbc.postgresql.org/download/postgresql-42.7.4.jar #driver for psql

# hive configuration ##

cd ../conf/
nano hive-site.xml
- paste configs:
<configuration>
	<property>
		<name>hive.server2.authentication</name>
		<value>NONE</value>
	</property>
	<property>
		<name>hive.metastore.warehouse.dir</name>
		<value>/user/hive/warehouse</value>
	</property>
	<property>
		<name>hive.server2.thrift.port</name>
		<value>5433</value>
		<description>TCP port number to listen on, default 10000</description>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		<value>jdbc:postgresql://team-25-nn:5432/metastore</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>org.postgresql.Driver</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>hive</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>any_pass_you_like</value>
	</property>
</configuration>

- quit

nano ~/.profile

-paste configs:
export HADOOP_HOME=/home/hadoop/hadoop-3.4.0
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

export HIVE_HOME=/home/hadoop/apache-hive-4.0.1-bin
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_AUX_JARS_PATH=$HIVE_HOME/lib/*
export PATH=$PATH:$HIVE_HOME/bin

- leave

source ~/.profile #activate environment

hive --version #check if hive works

cd $HADOOP_HOME/etc/hadoop
# Edit core-site.xml
nano core-site.xml
# paste configs
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://team-25-nn:9000</value>
    </property>
</configuration>
- Ñlose nano


## CREATING DIR FOR HIVE TO PUT DATA INTO ##
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -chmod g+w /tmp 
hdfs dfs -chmod g+w /user/hive/warehouse # granting rights


## INITIALIZING DB ## 
cd ~/apache-hive-4.0.1-bin/
bin/schematool -dbType postgres -initSchema


## LAUNCHING HIVE SERVER ##
hive --hiveconf hive.server2.enable.doAs=false --hiveconf hive.security.authorization.enabled=false --service hiveserver2 1>> /tmp/hs2.log 2>> /tmp/hs2.log &

## CREATING DB AND LOADING DATA ##
tmux

beeline -u jdbc:hive2://team-25-jn:5433

CREATE DATABASE test;
SHOW DATABASES; #check if test appeared

# get back to jn#
hdfs dfs -mkdir /input #create dir for data
hdfs dfs -chmod g+w /input #assign rights

# load data
cd ~
wget https://www.stats.govt.nz/assets/Uploads/Balance-of-payments/Balance-of-payments-and-international-investment-position-June-2024-quarter/Download-data/balance-of-payments-and-international-investment-position-june-2024-quarter.csv
mv balance-of-payments-and-international-investment-position-june-2024-quarter.csv test_file.csv # rename

hdfs dfs -put test_file.csv /input # from JN / hadoop, if doesn't work do source ~/.profile
hdfs fsck /input/test_file.csv
head -10 test_file.csv #see 10 first rows


## CREATING TABLE ##

beeline -u jdbc:hive2://team-25-jn:5433

use test; #go to previously created db


# Create table
CREATE TABLE IF NOT EXISTS test.balance_payments (
    Series_reference STRING,
    Period DECIMAL(6,2),
    Data_value INT,
    Suppressed STRING,
    STATUS STRING,
    UNITS STRING,
    MAGNTUDE INT,
    Subject STRING,
    Group_name STRING, 
    Series_title_1 STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES ("skip.header.line.count"="1");

# Show tables in the database
SHOW TABLES;

# Get table description
DESCRIBE balance_payments;

# Get detailed table information
SHOW CREATE TABLE balance_payments;

# Load data into table
LOAD DATA INPATH '/input/test_file.csv' INTO TABLE test.balance_payments;

- exit (ctr+C)

# check that data has loaded:
beeline -u jdbc:hive2://team-25-jn:5433
use test;

SELECT * FROM balance_payments LIMIT 5;
SELECT COUNT(*) FROM balance_payments;
- exit (ctr+C)



### Creating part table ###
beeline -u jdbc:hive2://team-25-jn:5433 # make sure it's hadoop user to edit SQL

# Create the partitioned table
CREATE TABLE test.balance_payments_partitioned (
    Series_reference STRING,
    Data_value INT,
    Suppressed STRING,
    STATUS STRING,
    UNITS STRING,
    MAGNTUDE INT,
    Subject STRING,
    Group_name STRING,
    Series_title_1 STRING
)
PARTITIONED BY (year INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';

# Set partition parameters
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;

# Insert data with partitioning
INSERT OVERWRITE TABLE test.balance_payments_partitioned 
PARTITION(year)
SELECT 
    Series_reference,
    Data_value,
    Suppressed,
    STATUS,
    UNITS,
    MAGNTUDE,
    Subject,
    Group_name,
    Series_title_1,
    CAST(FLOOR(Period) as INT) as year
FROM test.balance_payments;

# Verify part table
# Show partitions
SHOW PARTITIONS test.balance_payments_partitioned;

# Check some data
SELECT * FROM test.balance_payments_partitioned LIMIT 5;

- exit (ctrl+C)

### Creating part table ###

# check dir contents
hdfs dfs -ls /user/hive/warehouse/test.db/balance_payments

# see file contents
hdfs dfs -cat /user/hive/warehouse/test.db/balance_payments/* | head -n 5

## Non-coding part

# check Hadoop interface
# 1) SSH login (from local machine)
ssh -L 9870:localhost:9870 -L 8088:localhost:8088 -L 19888:localhost:19888 team@176.109.91.27

# 2) go to localhost:9870 for Hadoop

# Go to Utilities / Browse the file system and enter path:
/user/hive/warehouse/test.db/balance_payments/

# Check part table here:
/user/hive/warehouse/test.db/balance_payments_partitioned

#### HW3 ####




